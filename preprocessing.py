# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1plbFvCAKTbOFQs2qwg65DtPNwBGwwttz

## Pre-Processing
"""

import pandas as pd
import numpy as np
import re
import os
from datetime import datetime

def load_data(file_path):
    """
    Load data from an Excel or CSV file, specifically looking for Department and Feedback columns.

    Args:
        file_path (str): Path to the data file

    Returns:
        pd.DataFrame: Processed dataframe with standardized 'department' and 'free_text_comments' columns.
    """
    # Determine file type and load accordingly
    file_extension = os.path.splitext(file_path)[1].lower()

    if file_extension == '.xlsx' or file_extension == '.xls':
        df = pd.read_excel(file_path)
    elif file_extension == '.csv':
        df = pd.read_csv(file_path)
    else:
        raise ValueError(f"Unsupported file type: {file_extension}. Please provide a CSV or Excel file.")

    print(f"Original columns: {df.columns.tolist()}")
    df_columns_lower = {col.lower().strip(): col for col in df.columns} # Map lower case to original case

    # Define expected column name variants
    dept_variants = ['department', 'dept name', 'care group', 'dept']
    comment_variants = ['feedback', 'comments', 'narrative', 'free text', 'free_text_comments', 'freetext comments', 'narrative feedback']

    department_col = None
    comments_col = None

    # Find Department column
    for variant in dept_variants:
        if variant in df_columns_lower:
            department_col = df_columns_lower[variant]
            print(f"Found department column: '{department_col}'")
            break

    # Find Feedback/Comments column
    for variant in comment_variants:
        if variant in df_columns_lower:
            comments_col = df_columns_lower[variant]
            print(f"Found comments column: '{comments_col}'")
            break

    # Validate that both columns were found
    if not department_col:
        raise ValueError(f"Could not find a 'Department' column. Looked for variants like: {dept_variants}. Found columns: {df.columns.tolist()}")
    if not comments_col:
        raise ValueError(f"Could not find a 'Feedback' or 'Comments' column. Looked for variants like: {comment_variants}. Found columns: {df.columns.tolist()}")

    # Select and rename the required columns
    processed_df = df[[department_col, comments_col]].copy()
    processed_df.rename(columns={
        department_col: 'department',
        comments_col: 'free_text_comments'
    }, inplace=True)

    print(f"Standardized columns: {processed_df.columns.tolist()}")
    return processed_df

def preprocess_data(df):
    """
    Preprocess the dataframe: cleaning, normalization, and handling missing values

    Args:
        df (pd.DataFrame): Raw dataframe with department and free_text_comments

    Returns:
        pd.DataFrame: Cleaned and preprocessed dataframe
    """
    # Make a copy to avoid modifying the original
    processed_df = df.copy()

    # 1. Handle missing values
    # Drop rows with empty comments or department
    processed_df = processed_df.dropna(subset=['department', 'free_text_comments'])
    processed_df = processed_df[processed_df['free_text_comments'].astype(str).str.strip() != '']
    processed_df = processed_df[processed_df['department'].astype(str).str.strip() != '']

    # 2. Standardize department names
    processed_df['department'] = processed_df['department'].apply(
        lambda x: str(x).strip().title() if pd.notnull(x) else "Unknown"
    )

    # 3. Clean and normalize text comments
    processed_df['cleaned_comments'] = processed_df['free_text_comments'].apply(
        lambda x: clean_text(x) if pd.notnull(x) else ""
    )

    # 4. Add metadata
    processed_df['word_count'] = processed_df['cleaned_comments'].apply(lambda x: len(x.split()))
    processed_df['processing_date'] = datetime.now().strftime('%Y-%m-%d')

    # Keep original comment for reference if needed later
    processed_df['original_comment'] = df['free_text_comments']

    return processed_df

def clean_text(text):
    """
    Clean and normalize text data

    Args:
        text (str): Raw text

    Returns:
        str: Cleaned text
    """
    # Convert to string in case of non-string inputs
    text = str(text)

    # Convert to lowercase
    text = text.lower()

    # Remove special characters and digits (keep basic punctuation like apostrophes)
    text = re.sub(r'[^a-z0-9\s]', ' ', text) # Remove punctuation more aggressively if needed

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def main(file_path):
    """
    Main function to run the data preprocessing pipeline

    Args:
        file_path (str): Path to the data file

    Returns:
        pd.DataFrame: processed_df - preprocessed dataframe ready for NLP
    """
    print(f"Loading data from {file_path}...")
    raw_df = load_data(file_path)
    print(f"Loaded {len(raw_df)} rows of data.")

    print("Preprocessing data...")
    processed_df = preprocess_data(raw_df)
    print(f"After preprocessing: {len(processed_df)} rows remain.")

    # No longer returns dept_df from here
    return processed_df

# Example usage
if __name__ == "__main__":
    # This will be replaced with actual file path when running the code
    sample_file_path = "Sample.xlsx" # Make sure this file exists or change path
    try:
        processed_data = main(sample_file_path)

        # Display sample of the processed data
        print("\nSample of processed data:")
        print(processed_data[['department', 'cleaned_comments', 'word_count']].head())
    except FileNotFoundError:
        print(f"Error: Sample file '{sample_file_path}' not found.")
    except ValueError as ve:
        print(f"Error during preprocessing: {ve}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")