# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1plbFvCAKTbOFQs2qwg65DtPNwBGwwttz

## Pre-Processing
"""

import pandas as pd
import numpy as np
import re
import os
from datetime import datetime

def load_data(file_path):
    """
    Load data from an Excel or CSV file

    Args:
        file_path (str): Path to the data file

    Returns:
        pd.DataFrame: Processed dataframe with standardized columns
    """
    # Determine file type and load accordingly
    file_extension = os.path.splitext(file_path)[1].lower()

    if file_extension == '.xlsx' or file_extension == '.xls':
        df = pd.read_excel(file_path)
    elif file_extension == '.csv':
        df = pd.read_csv(file_path)
    else:
        raise ValueError(f"Unsupported file type: {file_extension}. Please provide a CSV or Excel file.")

    print(f"Original columns: {df.columns.tolist()}")
    
    # Handle single-question narrative files (like NQPS Q4)
    if len(df.columns) == 1 and 'department' not in [col.lower() for col in df.columns]:
        print(f"Detected single question format: {df.columns[0]}")
        # Create a dataframe with the narrative answers and assign a generic department
        comments_column = df.columns[0]
        processed_df = pd.DataFrame({
            'department': ['General'] * len(df),
            'free_text_comments': df[comments_column].values
        })
        return processed_df
    
    # Check if required columns exist (case insensitive)
    required_columns = ['department', 'free-text comments']
    df_columns_lower = [col.lower() for col in df.columns]

    # Map actual column names to standardized names
    column_mapping = {}
    for req_col in required_columns:
        matches = [col for i, col in enumerate(df.columns) if df_columns_lower[i] == req_col]
        if matches:
            column_mapping[matches[0]] = req_col.replace('-', '_').replace(' ', '_')
        else:
            # Try to find columns with similar names
            potential_matches = [
                col for i, col in enumerate(df.columns)
                if req_col.replace('-', '').replace(' ', '') in df_columns_lower[i].replace(' ', '')
            ]
            if potential_matches:
                column_mapping[potential_matches[0]] = req_col.replace('-', '_').replace(' ', '_')
            else:
                # If we can't find a department column, create one
                if req_col == 'department':
                    print(f"Department column not found. Creating a generic department column.")
                    df['department'] = 'General'
                    column_mapping['department'] = 'department'
                # If we can't find a comments column, try to use the first text column
                elif req_col == 'free-text comments':
                    # Look for columns that might contain comments
                    text_columns = [col for col in df.columns if 'comment' in col.lower() 
                                   or 'feedback' in col.lower() 
                                   or 'text' in col.lower()
                                   or 'response' in col.lower()
                                   or 'answer' in col.lower()]
                    
                    if text_columns:
                        print(f"Using {text_columns[0]} as comments column.")
                        column_mapping[text_columns[0]] = 'free_text_comments'
                    else:
                        # If no suitable column found, just use the first column as comments
                        print(f"Comment column not found. Using first column: {df.columns[0]}")
                        column_mapping[df.columns[0]] = 'free_text_comments'

    # Rename columns to standardized format
    df = df.rename(columns=column_mapping)

    return df

def preprocess_data(df):
    """
    Preprocess the dataframe: cleaning, normalization, and handling missing values

    Args:
        df (pd.DataFrame): Raw dataframe with department and free_text_comments

    Returns:
        pd.DataFrame: Cleaned and preprocessed dataframe
    """
    # Make a copy to avoid modifying the original
    processed_df = df.copy()

    # 1. Handle missing values
    # Drop rows with empty comments
    processed_df = processed_df.dropna(subset=['free_text_comments'])

    # 2. Standardize department names
    processed_df['department'] = processed_df['department'].apply(
        lambda x: str(x).strip().title() if pd.notnull(x) else "Unknown"
    )

    # 3. Clean and normalize text comments
    processed_df['cleaned_comments'] = processed_df['free_text_comments'].apply(
        lambda x: clean_text(x) if pd.notnull(x) else ""
    )

    # 4. Add metadata
    processed_df['word_count'] = processed_df['cleaned_comments'].apply(lambda x: len(x.split()))
    processed_df['processing_date'] = datetime.now().strftime('%Y-%m-%d')

    return processed_df

def clean_text(text):
    """
    Clean and normalize text data

    Args:
        text (str): Raw text

    Returns:
        str: Cleaned text
    """
    # Convert to string in case of non-string inputs
    text = str(text)

    # Convert to lowercase
    text = text.lower()

    # Remove special characters and digits
    text = re.sub(r'[^\w\s]', ' ', text)

    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text

def group_by_department(df):
    """
    Group comments by department and create department-level aggregations

    Args:
        df (pd.DataFrame): Preprocessed dataframe

    Returns:
        pd.DataFrame: Dataframe with department-level aggregations
    """
    dept_groups = df.groupby('department')

    # Create department-level aggregations
    dept_stats = pd.DataFrame({
        'comment_count': dept_groups['cleaned_comments'].count(),
        'avg_word_count': dept_groups['word_count'].mean().round(1),
        'all_comments': dept_groups['cleaned_comments'].apply(lambda x: ' '.join(x))
    }).reset_index()

    return dept_stats

def main(file_path):
    """
    Main function to run the data preprocessing pipeline

    Args:
        file_path (str): Path to the data file

    Returns:
        tuple: (raw_df, dept_df) - raw and department-level dataframes
    """
    print(f"Loading data from {file_path}...")
    raw_df = load_data(file_path)
    print(f"Loaded {len(raw_df)} rows of data.")

    print("Preprocessing data...")
    processed_df = preprocess_data(raw_df)
    print(f"After preprocessing: {len(processed_df)} rows remain.")

    print("Grouping by department...")
    dept_df = group_by_department(processed_df)
    print(f"Created department-level aggregations for {len(dept_df)} departments.")

    return processed_df, dept_df

# Example usage
if __name__ == "__main__":
    # This will be replaced with actual file path when running the code
    sample_file_path = "Sample.xlsx"
    processed_data, department_data = main(sample_file_path)

    # Display sample of the processed data
    print("\nSample of processed data:")
    print(processed_data.head())

    print("\nDepartment-level aggregations:")
    print(department_data.head())