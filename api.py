# -*- coding: utf-8 -*-
"""api.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10kXSLX7Q7tqxDi38orC4i3VRDSet7jYA
"""

# api.py - FastAPI backend
from fastapi import FastAPI, HTTPException, Query, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import json
import os
import tempfile
from datetime import datetime

# Import the processing functions from previous stages
# In a real-world scenario, these would be properly imported from the respective modules
from preprocessing import load_data, preprocess_data, group_by_department
from nlp_pipeline import process_nlp_pipeline, NLPProcessor
# Optional import for the predictive model
try:
    from predictive_model import run_predictive_modeling
except ImportError:
    print("Predictive model module not available")
    run_predictive_modeling = None

# Define data models for API responses
class ThemeContribution(BaseModel):
    feature: str
    contribution_pct: int

class DepartmentRisk(BaseModel):
    department: str
    risk_score: float
    high_risk: bool
    top_themes: List[Dict[str, Any]] = []

class DepartmentList(BaseModel):
    departments: List[str]

# Initialize FastAPI app
app = FastAPI(
    title="Staff Feedback Analysis API",
    description="API for analyzing staff feedback and identifying department risk levels",
    version="1.0.0"
)

# Enable CORS for frontend integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Store processed data in memory (for the PoC)
# In a production environment, use a proper database
app_data = {
    "raw_data": None,
    "processed_data": None,
    "department_data": None,
    "risk_scores": None,
    "explanations": None,
    "last_updated": None
}

@app.get("/", tags=["Info"])
async def root():
    """API root endpoint"""
    return {
        "message": "Staff Feedback Analysis API",
        "status": "active",
        "data_loaded": app_data["raw_data"] is not None,
        "last_updated": app_data["last_updated"]
    }

@app.post("/upload", tags=["Data"])
async def upload_data(file: UploadFile = File(...)):
    """Upload and process new data file"""
    if not file:
        raise HTTPException(status_code=400, detail="No file provided")

    # Check file extension
    file_ext = os.path.splitext(file.filename)[1].lower()
    if file_ext not in ['.csv', '.xlsx', '.xls']:
        raise HTTPException(status_code=400, detail="Unsupported file format. Please upload a CSV or Excel file.")

    try:
        # Save uploaded file to temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as temp_file:
            content = await file.read()
            if not content:
                raise HTTPException(status_code=400, detail="Empty file provided")

            temp_file.write(content)
            temp_path = temp_file.name

        # Process the uploaded file
        try:
            raw_df = load_data(temp_path)
            if raw_df is None or raw_df.empty:
                raise ValueError("Unable to load data from file or file is empty")
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Error loading data: {str(e)}")

        try:
            processed_df = preprocess_data(raw_df)
            comment_df = process_nlp_pipeline(processed_df)
            
            # Generate department data using NLPProcessor's aggregate_by_department method
            nlp_processor = NLPProcessor()
            dept_df = nlp_processor.aggregate_by_department(comment_df)
            
            # Skip predictive modeling for now
            # risk_scores, explanations = run_predictive_modeling(dept_df)
            risk_scores, explanations = None, None
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error processing data: {str(e)}")

        # Calculate comments_processed and departments_processed
        comments_processed = len(processed_df) if processed_df is not None else 0
        departments_processed = len(dept_df) if dept_df is not None else 0

        # Store the processed data
        app_data["raw_data"] = raw_df
        app_data["processed_data"] = comment_df  # Store the NLP-processed data with sentiment_score
        app_data["department_data"] = dept_df
        app_data["risk_scores"] = dept_df  # Use department data instead of risk scores
        app_data["explanations"] = None
        app_data["last_updated"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Clean up temporary file
        os.unlink(temp_path)

        return {
            "status": "success",
            "file_name": file.filename,
            "comments_processed": comments_processed,
            "departments_processed": departments_processed,
        }

    except HTTPException:
        # Re-raise HTTP exceptions
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")

@app.get("/departments", response_model=DepartmentList, tags=["Data"])
async def get_departments():
    """Get list of all departments"""
    if app_data["risk_scores"] is None:
        raise HTTPException(status_code=404, detail="No data loaded. Please upload data first.")

    departments = app_data["risk_scores"]["department"].tolist()
    return {"departments": departments}

@app.get("/risk/all", response_model=List[DepartmentRisk], tags=["Risk Analysis"])
async def get_all_risks():
    """Get risk scores for all departments"""
    if app_data["risk_scores"] is None:
        raise HTTPException(status_code=404, detail="No data loaded. Please upload data first.")

    result = []
    try:
        for _, row in app_data["risk_scores"].iterrows():
            dept = row.get("department", "Unknown")
            if not dept or dept == "Unknown":
                continue  # Skip entries without valid department names

            # Use sentiment score as a proxy for risk
            # Higher negative sentiment = higher risk
            sentiment = row.get("avg_sentiment", 0.5)
            # Invert the sentiment score (1 - sentiment) to get risk score (0-1)
            # Then scale to 1-5 range
            risk_score = ((1 - sentiment) * 4) + 1
            
            # Define high risk as having risk score > 3.5
            high_risk = risk_score > 3.5

            # Get top themes with highest counts
            theme_cols = [col for col in row.index if col.endswith('_count')]
            top_themes = []
            
            if theme_cols:
                themes_data = [(col.replace('_count', ''), row[col]) for col in theme_cols]
                # Sort by count, descending
                themes_data.sort(key=lambda x: x[1], reverse=True)
                
                # Take top 3 themes
                for i, (theme, count) in enumerate(themes_data[:3]):
                    if count > 0:  # Only include themes that were mentioned
                        # Calculate contribution percentage (out of total comments)
                        comment_count = row.get("comment_count", 1)  # Avoid division by zero
                        contribution_pct = int((count / comment_count) * 100) if comment_count > 0 else 0
                        
                        top_themes.append({
                            "feature": theme,
                            "contribution_pct": contribution_pct
                        })

            result.append({
                "department": dept,
                "risk_score": float(risk_score),
                "high_risk": bool(high_risk),
                "top_themes": top_themes
            })

        # Sort by risk score (highest risk first)
        result.sort(key=lambda x: x["risk_score"], reverse=True)
        return result

    except Exception as e:
        print(f"Error in get_all_risks: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving risk data: {str(e)}")

@app.get("/risk/{department}", response_model=DepartmentRisk, tags=["Risk Analysis"])
async def get_department_risk(department: str):
    """Get risk score for a specific department"""
    if app_data["risk_scores"] is None:
        raise HTTPException(status_code=404, detail="No data loaded. Please upload data first.")

    # Find department in risk scores
    dept_risk = app_data["risk_scores"][app_data["risk_scores"]["department"] == department]
    if dept_risk.empty:
        raise HTTPException(status_code=404, detail=f"Department '{department}' not found.")

    try:
        # Get row data
        risk_data = dept_risk.iloc[0]
        
        # Use sentiment score as a proxy for risk
        sentiment = risk_data.get("avg_sentiment", 0.5)
        # Invert the sentiment score (1 - sentiment) to get risk score (0-1)
        # Then scale to 1-5 range
        risk_score = ((1 - sentiment) * 4) + 1
        
        # Define high risk as having risk score > 3.5
        high_risk = risk_score > 3.5

        # Get top themes with highest counts
        theme_cols = [col for col in risk_data.index if col.endswith('_count')]
        top_themes = []
        
        if theme_cols:
            themes_data = [(col.replace('_count', ''), risk_data[col]) for col in theme_cols]
            # Sort by count, descending
            themes_data.sort(key=lambda x: x[1], reverse=True)
            
            # Take top 3 themes
            for i, (theme, count) in enumerate(themes_data[:3]):
                if count > 0:  # Only include themes that were mentioned
                    # Calculate contribution percentage (out of total comments)
                    comment_count = risk_data.get("comment_count", 1)  # Avoid division by zero
                    contribution_pct = int((count / comment_count) * 100) if comment_count > 0 else 0
                    
                    top_themes.append({
                        "feature": theme,
                        "contribution_pct": contribution_pct
                    })

        return {
            "department": department,
            "risk_score": float(risk_score),
            "high_risk": bool(high_risk),
            "top_themes": top_themes
        }
    except Exception as e:
        print(f"Error in get_department_risk: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving department risk: {str(e)}")

@app.get("/themes/summary", tags=["Theme Analysis"])
async def get_themes_summary():
    """Get summary of themes across all departments"""
    if app_data["department_data"] is None:
        raise HTTPException(status_code=404, detail="No data loaded. Please upload data first.")

    try:
        # Get theme columns (those ending with _count)
        theme_cols = [col for col in app_data["department_data"].columns if col.endswith('_count')]

        # Calculate total counts for each theme
        theme_totals = {}
        for col in theme_cols:
            theme_name = col.replace('_count', '')
            theme_totals[theme_name] = int(app_data["department_data"][col].sum())

        # Sort by count (descending)
        sorted_themes = sorted(theme_totals.items(), key=lambda x: x[1], reverse=True)

        return {"themes": dict(sorted_themes)}
    except Exception as e:
        print(f"Error in get_themes_summary: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating themes summary: {str(e)}")

@app.get("/comments/sample", tags=["Data"])
async def get_comment_samples(department: Optional[str] = None, theme: Optional[str] = None, limit: int = 5):
    """Get sample comments, optionally filtered by department and/or theme"""
    if app_data["processed_data"] is None:
        raise HTTPException(status_code=404, detail="No data loaded. Please upload data first.")

    # Start with all processed data
    df = app_data["processed_data"]
    
    # Debug information
    print(f"Columns available in processed_data: {df.columns.tolist()}")
    
    # Filter by department if specified
    if department:
        df = df[df['department'] == department]
        if df.empty:
            raise HTTPException(status_code=404, detail=f"Department '{department}' not found.")

    # Filter by theme if specified
    if theme:
        theme_col = f'theme_{theme}'
        if theme_col not in df.columns:
            raise HTTPException(status_code=404, detail=f"Theme '{theme}' not found.")
        df = df[df[theme_col] == 1]
        if df.empty:
            raise HTTPException(status_code=404, detail=f"No comments found for theme '{theme}'.")

    try:
        # Check what columns are available
        available_columns = ['department', 'free_text_comments']
        if 'sentiment_score' in df.columns:
            available_columns.append('sentiment_score')
        
        # Get sample comments
        samples = df.sample(min(limit, len(df)))[available_columns].to_dict(orient='records')
        return {"samples": samples, "total_matching": len(df)}
    except Exception as e:
        print(f"Error in get_comment_samples: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving comments: {str(e)}")

@app.get("/data/summary", tags=["Data"])
async def get_data_summary():
    """Get summary statistics of the loaded data"""
    if app_data["raw_data"] is None:
        raise HTTPException(status_code=404, detail="No data loaded. Please upload data first.")

    try:
        # Basic summary stats
        raw_data_len = len(app_data["raw_data"]) if app_data["raw_data"] is not None else 0
        dept_data_len = len(app_data["department_data"]) if app_data["department_data"] is not None else 0

        # Calculate average safely
        avg_comments = 0
        if dept_data_len > 0:
            avg_comments = round(raw_data_len / dept_data_len, 1)

        # Get sentiment counts safely based on sentiment_score
        sentiment_counts = {"positive": 0, "neutral": 0, "negative": 0}
        if app_data["processed_data"] is not None:
            try:
                # Categorize sentiment scores: > 0.6 is positive, < 0.4 is negative, rest is neutral
                sentiment_counts["positive"] = int((app_data["processed_data"]["sentiment_score"] > 0.6).sum())
                sentiment_counts["neutral"] = int(((app_data["processed_data"]["sentiment_score"] >= 0.4) & 
                                                  (app_data["processed_data"]["sentiment_score"] <= 0.6)).sum())
                sentiment_counts["negative"] = int((app_data["processed_data"]["sentiment_score"] < 0.4).sum())
            except Exception as e:
                print(f"Error counting sentiments: {str(e)}")
                # If there's an error counting sentiments, keep defaults
                pass

        # Get high risk count safely - using inverted sentiment score
        high_risk_count = 0
        if app_data["department_data"] is not None:
            try:
                # High risk is when avg_sentiment < 0.4 (which means risk_score > 3.5)
                high_risk_count = int((app_data["department_data"]["avg_sentiment"] < 0.4).sum())
            except Exception as e:
                print(f"Error counting high risk: {str(e)}")
                # If there's an error counting high risk, keep default
                pass

        summary = {
            "total_comments": raw_data_len,
            "total_departments": dept_data_len,
            "avg_comments_per_department": avg_comments,
            "sentiment_distribution": sentiment_counts,
            "high_risk_departments": high_risk_count,
            "last_updated": app_data["last_updated"]
        }

        return summary
    except Exception as e:
        print(f"Error in get_data_summary: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating data summary: {str(e)}")

# Run the app with: uvicorn api:app --reload
# For the PoC, this would be imported and used by the Streamlit app