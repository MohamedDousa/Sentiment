# -*- coding: utf-8 -*-
"""nlp_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VgKOq01wR6M6TThpITSwJ5cHGXicnuhh
"""

import spacy
from collections import Counter
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline
import re
import pandas as pd
import torch
class NLPProcessor:
    def __init__(self, themes_dict=None):
        """
        Initialize the NLP processor with sentiment analysis and theme extraction capabilities

        Args:
            themes_dict (dict, optional): Dictionary mapping themes to keywords
        """
        # Define default themes if not provided
        self.themes = themes_dict or {
            # Existing themes with expansions
            "workload": ["overwhelming", "unsustainable", "long hours", "exhausted",
                        "pressure", "unmanageable", "stress", "burnout", "capacity",
                        "demanding", "relentless", "backlog", "targets", "ratio"],

            "staffing": ["understaffed", "shortages", "recruitment", "retention",
                        "agency staff", "bank shifts", "rota gaps", "sickness absence",
                        "vacancies", "skill mix", "skill drain", "agency rates"],

            "management": ["supportive", "unresponsive", "micromanagement",
                          "leadership", "transparency", "consultation", "trust",
                          "favouritism", "nepotism", "autocratic", "bureaucracy"],

            "facilities": ["outdated equipment", "leaking roof", "asbestos",
                          "infrastructure", "ventilation", "heating", "lighting",
                          "infection control", "ward space", "privacy", "crowding"],

            # New critical themes from sample data
            "career_development": [
                "band progression", "dead-end job", "promotion", "apprenticeship",
                "career stagnation", "training budget", "qualifications",
                "skill recognition", "pay scales", "agenda for change", "CEPD"
            ],

            "compensation": [
                "cost of living", "salary freeze", "pay disparity", "London weighting",
                "bank rates", "overtime pay", "pension deductions", "food banks",
                "financial stress", "living wage", "tax burden", "overtime ban"
            ],

            "work_environment": [
                "bullying culture", "racial discrimination", "harassment",
                "fear of speaking up", "toxic atmosphere", "cliques", "ostracized",
                "discrimination", "racism", "BAME", "ethnic minority", "equity"
            ],

            "patient_care_impact": [
                "corridor nursing", "missed observations", "medication errors",
                "delayed discharges", "waiting lists", "cancelled clinics",
                "safety incidents", "near misses", "DATIX", "patient harm",
                "breaches", "12hr waits"
            ],

            "flexibility_worklife": [
                "childcare conflicts", "school run", "elder care", "shift patterns",
                "work-life balance", "remote work", "agile working", "WFH",
                "compressed hours", "part-time options", "job share"
            ],

            "IT_systems": [
                "electronic prescribing", "legacy systems", "CareFlow", "Trak",
                "login issues", "system downtime", "interoperability", "EPR",
                "digital notes", "cyber security", "outdated software"
            ],

            "training_support": [
                "preceptorship", "mandatory training", "supernumerary time",
                "clinical supervision", "mentorship", "CPD", "simulation training",
                "induction", "competency framework", "protected learning time"
            ],

            "estates_logistics": [
                "parking permits", "staff transport", "site access", "security",
                "smoking areas", "canteen prices", "vending machines", "locker space",
                "changing rooms", "shuttle service", "bike storage"
            ],

            "wellbeing_services": [
                "occupational health", "counselling", "resilience training",
                "mental health first aid", "health checks", "fitness facilities",
                "stress management", "trauma support", "financial advice"
            ],

            "governance_risk": [
                "whistleblowing", "FTSU guardians", "SI investigations",
                "risk assessments", "compliance", "CQC ratings", "audits",
                "policy adherence", "documentation burden", "reporting culture"
            ],

            "team_dynamics": [
                "interdepartmental conflict", "MDT working", "professional respect",
                "nurse-doctor relations", "support workers", "skill mix tension",
                "agency staff integration", "bank staff treatment", "team morale"
            ],

            "resource_allocation": [
                "bed management", "theatre time", "MRI slots", "clinic space",
                "bank staff costs", "agency spend", "outsourcing", "homecare",
                "equipment budgets", "drug shortages", "supply chain"
            ]
        }

        # Enhanced exclusion patterns to reduce false positives
        self.exclusion_patterns = {
            "workload": [r"workload balancing", r"workload analysis"],
            "management": [r"time management", r"project management"],
            "career_development": [r"career day", r"career fair"],
            "compensation": [r"compensation claim", r"injury compensation"],
            "work_environment": [r"work environment scheme", r"environmental health"],
            "IT_systems": [r"IT department", r"IT technician"],
            "training_support": [r"training room", r"training venue"],
            "estates_logistics": [r"real estate", r"estate planning"],
            "governance_risk": [r"corporate governance", r"board governance"]
        }

        # Load spaCy model for text processing
        print("Loading spaCy model...")
        self.nlp = spacy.load("en_core_web_sm")

        # Initialize Hugging Face sentiment analysis pipeline
        print("Loading sentiment analysis model...")
        self.sentiment_analyzer = pipeline(
            "sentiment-analysis",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            tokenizer="distilbert-base-uncased-finetuned-sst-2-english",
            return_all_scores=True
        )

        # Preprocess themes and create refined patterns
        self._preprocess_themes()
        print("NLP processor initialized.")

    def _preprocess_themes(self):
        """Preprocess themes for efficient matching with inclusion and exclusion patterns"""
        # Convert all keywords to lowercase
        self.themes = {theme: [kw.lower() for kw in keywords]
                       for theme, keywords in self.themes.items()}

        # Create refined theme patterns with include and exclude patterns
        self.refined_patterns = {}

        for theme, keywords in self.themes.items():
            # Sort keywords by length (longest first) to prioritize longer matches
            sorted_keywords = sorted(keywords, key=len, reverse=True)

            # Create include pattern (positive matches)
            include_pattern = r'\b(' + '|'.join(re.escape(kw) for kw in sorted_keywords) + r')\b'
            include_regex = re.compile(include_pattern, re.IGNORECASE)

            # Create exclude pattern if available for this theme
            exclude_regex = None
            if theme in self.exclusion_patterns and self.exclusion_patterns[theme]:
                exclude_pattern = r'\b(' + '|'.join(re.escape(ex) for ex in self.exclusion_patterns[theme]) + r')\b'
                exclude_regex = re.compile(exclude_pattern, re.IGNORECASE)

            # Store both patterns
            self.refined_patterns[theme] = {
                'include': include_regex,
                'exclude': exclude_regex
            }

    def analyze_sentiment(self, text):
        """
        Analyze sentiment of a text using Hugging Face transformers

        Args:
            text (str): Text to analyze

        Returns:
            float: Sentiment score between 0 and 1 (0 = negative, 1 = positive)
        """
        if not text or len(text.strip()) == 0:
            return 0.5

        try:
            # Get all sentiment scores
            results = self.sentiment_analyzer(text)

            # Format results into the expected output
            # Hugging Face returns labels as POSITIVE/NEGATIVE, convert to match required format
            scores = {item['label']: item['score'] for item in results[0]}

            if 'POSITIVE' in scores:
                return scores['POSITIVE']
            else:
                return 0.5  # Default to neutral if POSITIVE not found
        except Exception as e:
            print(f"Error analyzing sentiment for text: {text[:100]}... Error: {str(e)}")
            return 0.5  # Default to neutral on error

    def extract_themes(self, text):
        """
        Extract themes from text using refined rule-based matching with exclusions

        Args:
            text (str): Text to extract themes from

        Returns:
            list: List of identified themes
        """
        if not text or len(text.strip()) == 0:
            return []

        text = text.lower()
        found_themes = []

        # Use refined patterns with include/exclude logic
        for theme, patterns in self.refined_patterns.items():
            # Check if text matches include pattern
            if patterns['include'].search(text):
                # Check if we should exclude this match
                if patterns['exclude'] and patterns['exclude'].search(text):
                    # Skip if exclusion pattern matches
                    continue
                found_themes.append(theme)

        return found_themes

    def process_comment(self, text):
        """
        Process a single comment to extract sentiment and themes

        Args:
            text (str): Text comment to process

        Returns:
            dict: Dictionary with sentiment score and themes
        """
        # Default values
        sentiment_score = 0.5  # Neutral by default
        themes = []

        try:
            # Only analyze text that has content
            if text and len(text.strip()) > 0:
                # Truncate text if it's too long for the model
                # The sentiment model has a limit of 512 tokens
                if len(text) > 1500:  # Approximate character count for 512 tokens
                    print(f"Warning: Text too long ({len(text)} chars), truncating for sentiment analysis.")
                    truncated_text = text[:1500]
                else:
                    truncated_text = text
                
                # Get sentiment
                sentiment_score = self.analyze_sentiment(truncated_text)
                
                # Extract themes
                themes = self.extract_themes(text)
        except Exception as e:
            print(f"Error processing comment: {str(e)}")
            # Keep default values in case of error
            
        # Convert themes to dictionary
        theme_dict = {theme: 1 for theme in themes}
        
        return {
            'sentiment': sentiment_score,
            'themes': theme_dict
        }

    def process_dataframe(self, df, comment_col='free_text_comments'):
        """
        Process all comments in a dataframe

        Args:
            df (pd.DataFrame): Dataframe with comments
            comment_col (str): Name of the column containing comments

        Returns:
            pd.DataFrame: Dataframe with sentiment and theme columns
        """
        # Create a copy to avoid modifying the original
        result_df = df.copy()

        # Initialize sentiment and theme columns
        result_df['sentiment_score'] = 0.5
        
        # Initialize theme columns
        for theme in self.themes.keys():
            result_df[f'theme_{theme}'] = 0

        # Process each comment
        for idx, row in result_df.iterrows():
            comment = row[comment_col]

            # Skip empty comments
            if pd.isnull(comment) or comment == "":
                continue

            # Process the comment
            processed = self.process_comment(comment)

            # Update the dataframe
            result_df.at[idx, 'sentiment_score'] = processed['sentiment']

            # Update theme columns
            for theme in processed['themes']:
                result_df.at[idx, f'theme_{theme}'] = 1

        return result_df

    def update_theme_rules(self, theme, add_keywords=None, remove_keywords=None, add_exclusions=None):
        """
        Update rules for a specific theme

        Args:
            theme (str): Theme to update
            add_keywords (list): Keywords to add to theme
            remove_keywords (list): Keywords to remove from theme
            add_exclusions (list): Exclusion patterns to add
        """
        # Create theme if it doesn't exist
        if theme not in self.themes:
            if add_keywords:
                self.themes[theme] = []
            else:
                raise ValueError(f"Theme '{theme}' not found and no keywords provided")

        # Update include keywords
        if add_keywords:
            self.themes[theme].extend([kw for kw in add_keywords if kw not in self.themes[theme]])

        if remove_keywords:
            self.themes[theme] = [kw for kw in self.themes[theme] if kw not in remove_keywords]

        # Update exclusions
        if add_exclusions:
            if theme not in self.exclusion_patterns:
                self.exclusion_patterns[theme] = []

            self.exclusion_patterns[theme].extend([ex for ex in add_exclusions
                                                  if ex not in self.exclusion_patterns[theme]])

        # Regenerate patterns after updates
        self._preprocess_themes()
        print(f"Theme '{theme}' rules updated successfully")

    def aggregate_by_department(self, df, dept_col='department'):
        """
        Aggregate NLP results by department

        Args:
            df (pd.DataFrame): Dataframe with processed comments
            dept_col (str): Name of the column containing department names

        Returns:
            pd.DataFrame: Dataframe with department-level aggregations
        """
        # Group by department
        dept_groups = df.groupby(dept_col)

        # Prepare aggregation dictionary
        agg_dict = {
            'sentiment_score': 'mean',
            'cleaned_comments': 'count'
        }

        # Add theme columns to aggregation
        for theme in self.themes:
            agg_dict[f'theme_{theme}'] = 'sum'

        # Create department-level aggregations
        dept_stats = dept_groups.agg(agg_dict).reset_index()

        # Rename columns for clarity
        dept_stats = dept_stats.rename(columns={
            'cleaned_comments': 'comment_count',
            'sentiment_score': 'avg_sentiment'
        })

        # Calculate percentages for each theme
        for theme in self.themes:
            # Calculate raw count and percentage
            theme_col = f'theme_{theme}'
            percent_col = f'{theme}_percent'
            dept_stats[percent_col] = (dept_stats[theme_col] / dept_stats['comment_count'] * 100).round(1)

            # Rename the raw count column
            dept_stats = dept_stats.rename(columns={theme_col: f'{theme}_count'})

        # Normalize sentiment score to 0-1 range (if not already)
        if dept_stats['avg_sentiment'].max() > 1:
            dept_stats['avg_sentiment'] = dept_stats['avg_sentiment'] / 100

        # Round sentiment score
        dept_stats['avg_sentiment'] = dept_stats['avg_sentiment'].round(2)

        return dept_stats

def process_nlp_pipeline(df, themes_dict=None, exclusion_patterns=None):
    """
    Run the complete NLP pipeline on a preprocessed dataframe

    Args:
        df (pd.DataFrame): Preprocessed dataframe
        themes_dict (dict, optional): Dictionary of themes and keywords
        exclusion_patterns (dict, optional): Dictionary of exclusion patterns

    Returns:
        pd.DataFrame: Dataframe with sentiment and theme analysis
    """
    # Initialize NLP processor
    nlp_processor = NLPProcessor(themes_dict)

    # Add exclusion patterns if provided
    if exclusion_patterns:
        for theme, patterns in exclusion_patterns.items():
            nlp_processor.update_theme_rules(theme, add_exclusions=patterns)

    print("Processing comments...")
    comment_df = nlp_processor.process_dataframe(df)
    
    # Verify sentiment_score column exists
    if 'sentiment_score' not in comment_df.columns:
        print("Warning: sentiment_score column not found, adding default values")
        comment_df['sentiment_score'] = 0.5
    
    # Debug output
    print(f"Processed {len(comment_df)} comments.")
    print(f"Columns in processed data: {comment_df.columns.tolist()}")
    
    # Ensure we have at least one row to verify structure
    if len(comment_df) > 0:
        first_row = comment_df.iloc[0]
        print(f"Sample sentiment_score: {first_row.get('sentiment_score', 'NOT FOUND')}")
        
        # Check theme columns
        theme_cols = [col for col in comment_df.columns if col.startswith('theme_')]
        print(f"Theme columns: {theme_cols}")

    return comment_df

# Example usage (would be integrated with the previous preprocessing code)
if __name__ == "__main__":
    # Assuming processed_data from the previous stage is available
    # This would be replaced with actual file path and processing when running the code
    sample_file_path = "staff_feedback.xlsx"

    # Sample exclusion patterns
    sample_exclusions = {
        "workload": ["workflow", "work load balancing"],
        "staffing": ["staff meeting", "staffing agency"],
        "management": ["time management", "project management"],
        "career_development": ["career day", "career fair"],
        "compensation": ["compensation claim", "injury compensation"],
        "work_environment": ["work environment scheme", "environmental health"],
        "IT_systems": ["IT department", "IT technician"],
        "training_support": ["training room", "training venue"],
        "estates_logistics": ["real estate", "estate planning"],
        "governance_risk": ["corporate governance", "board governance"]
    }

    # Run NLP pipeline with exclusions
    comment_level_results = process_nlp_pipeline(
        processed_data,
        exclusion_patterns=sample_exclusions
    )

    # Display sample results
    print("\nSample of comment-level results:")
    print(comment_level_results[['department', 'cleaned_comments', 'sentiment', 'sentiment_score'] +
                             [f'theme_{theme}' for theme in NLPProcessor().themes]].head())

    print("\nDepartment-level aggregations:")
    print(comment_level_results.head())