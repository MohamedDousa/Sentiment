# -*- coding: utf-8 -*-
"""predictive_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SEuWtiZBxGvWVMEU1P21wNUirmFKFItG
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import DBSCAN
import xgboost as xgb
import shap
import warnings
warnings.filterwarnings('ignore')

class PredictiveModel:
    def __init__(self, risk_threshold=0.6):
        """
        Initialize the predictive model with XGBoost and SHAP explanations

        Args:
            risk_threshold (float): Threshold for high-risk classification (0-1)
        """
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        self.risk_threshold = risk_threshold
        self.explainer = None
        print("Predictive model initialized.")

    def prepare_features(self, df, label_col=None):
        """
        Prepare features for modeling from department-level data

        Args:
            df (pd.DataFrame): Department-level aggregated data
            label_col (str, optional): Name of column containing actual risk labels (if available)

        Returns:
            tuple: (X, y, departments) - features, labels, and department names
        """
        # Store department names
        departments = df['department'].values

        # Define feature columns (excluding non-feature columns)
        non_feature_cols = ['department', 'all_comments']
        if label_col:
            non_feature_cols.append(label_col)

        # Select feature columns
        feature_cols = [col for col in df.columns if col not in non_feature_cols]
        
        # Ensure all feature columns contain numeric data
        numeric_df = df.copy()
        for col in feature_cols:
            if not pd.api.types.is_numeric_dtype(numeric_df[col]):
                print(f"Converting non-numeric column {col} to numeric")
                # For text columns, we'll just use the length as a feature
                if pd.api.types.is_string_dtype(numeric_df[col]):
                    numeric_df[col] = numeric_df[col].str.len()
                # For other non-numeric types, convert to NaN and then fill with 0
                else:
                    numeric_df[col] = pd.to_numeric(numeric_df[col], errors='coerce').fillna(0)
        
        self.feature_names = feature_cols

        # Extract features
        X = numeric_df[feature_cols].values

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        # Extract labels if available
        y = df[label_col].values if label_col else None

        return X_scaled, y, departments

    def train_model(self, X, y=None):
        """
        Train an XGBoost model if labels are available

        Args:
            X (np.ndarray): Feature matrix
            y (np.ndarray, optional): Labels

        Returns:
            self: The trained model instance
        """
        if y is None:
            print("No labels provided. Skipping model training.")
            return self

        print("Training XGBoost model...")
        # Initialize XGBoost regressor
        self.model = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=100,
            max_depth=4,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )

        # Train the model
        self.model.fit(X, y)

        # Initialize SHAP explainer
        self.explainer = shap.TreeExplainer(self.model)

        print("Model training complete.")
        return self

    def evaluate_model(self, X, y):
        """
        Evaluate the trained model using cross-validation

        Args:
            X (np.ndarray): Feature matrix
            y (np.ndarray): Labels

        Returns:
            dict: Evaluation metrics
        """
        if self.model is None:
            print("Model not trained. Please train the model first.")
            return {}

        print("Evaluating model with cross-validation...")
        # Perform cross-validation
        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

        # For stratification, we need to convert regression targets to classes
        y_classes = pd.qcut(y, q=5, labels=False)

        # Calculate cross-validation scores
        cv_scores = cross_val_score(self.model, X, y, cv=kfold, scoring='neg_mean_absolute_error')

        # Calculate metrics on full dataset
        y_pred = self.model.predict(X)
        mse = mean_squared_error(y, y_pred)
        mae = mean_absolute_error(y, y_pred)
        r2 = r2_score(y, y_pred)

        metrics = {
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'cv_mae': -np.mean(cv_scores)
        }

        print(f"Evaluation metrics: MSE={mse:.4f}, MAE={mae:.4f}, R2={r2:.4f}, CV-MAE={-np.mean(cv_scores):.4f}")
        return metrics

    def predict_risk_scores(self, X, departments=None):
        """
        Predict risk scores for departments

        Args:
            X (np.ndarray): Feature matrix
            departments (np.ndarray, optional): Department names

        Returns:
            pd.DataFrame: Dataframe with departments and risk scores
        """
        # If model is available, use it for prediction
        if self.model is not None:
            print("Predicting risk scores using trained model...")
            risk_scores = self.model.predict(X)
        else:
            # If no model, use clustering for risk score estimation
            print("No trained model found. Using clustering for risk estimation...")
            risk_scores = self._cluster_based_scoring(X)

        # Scale risk scores to 1-5 range
        risk_scores_scaled = self._scale_to_range(risk_scores, min_val=1, max_val=5)

        # Create dataframe with departments and risk scores
        results = pd.DataFrame({
            'department': departments if departments is not None else [f"Dept_{i}" for i in range(len(X))],
            'risk_score': risk_scores_scaled.round(1)
        })

        # Add risk flag
        threshold = 3.5  # Threshold on the 1-5 scale (corresponds to ~0.6 on 0-1 scale)
        results['high_risk'] = results['risk_score'] >= threshold

        print(f"Predicted risk scores for {len(results)} departments.")
        return results

    def _cluster_based_scoring(self, X):
        """
        Use DBSCAN clustering to identify outliers for risk scoring when no labels are available

        Args:
            X (np.ndarray): Feature matrix

        Returns:
            np.ndarray: Risk scores based on outlier detection
        """
        # Apply DBSCAN clustering
        dbscan = DBSCAN(eps=0.5, min_samples=3)
        clusters = dbscan.fit_predict(X)

        # Calculate distances from each point to its nearest neighbors
        from sklearn.neighbors import NearestNeighbors
        nn = NearestNeighbors(n_neighbors=3)
        nn.fit(X)
        distances, _ = nn.kneighbors(X)
        avg_distances = distances.mean(axis=1)

        # Normalize distances to 0-1 range
        risk_scores = (avg_distances - avg_distances.min()) / (avg_distances.max() - avg_distances.min())

        # Identify outliers (points with cluster label -1)
        outliers = clusters == -1

        # Boost risk scores for outliers
        risk_scores[outliers] = np.maximum(risk_scores[outliers], 0.7)

        return risk_scores

    def _scale_to_range(self, values, min_val=1, max_val=5):
        """
        Scale values to a specified range

        Args:
            values (np.ndarray): Values to scale
            min_val (float): Minimum value in new range
            max_val (float): Maximum value in new range

        Returns:
            np.ndarray: Scaled values
        """
        old_min, old_max = np.min(values), np.max(values)

        # Handle the case where all values are the same
        if old_min == old_max:
            return np.full_like(values, (min_val + max_val) / 2)

        # Scale to new range
        return min_val + (values - old_min) * (max_val - min_val) / (old_max - old_min)

    def get_shap_explanations(self, X, departments=None, top_n=3):
        """
        Generate SHAP explanations for risk scores

        Args:
            X (np.ndarray): Feature matrix
            departments (np.ndarray, optional): Department names
            top_n (int): Number of top features to include in explanation

        Returns:
            pd.DataFrame: Dataframe with SHAP explanations for each department
        """
        if self.model is None or self.explainer is None:
            print("Model or explainer not available. Cannot generate SHAP explanations.")
            return []

        print("Generating SHAP explanations...")
        # Calculate SHAP values
        shap_values = self.explainer.shap_values(X)

        # Create explanations for each department
        explanations = []
        for i in range(len(X)):
            # Get department name
            dept = departments[i] if departments is not None else f"Dept_{i}"

            # Get risk score
            risk_score = self.model.predict([X[i]])[0]
            risk_score_scaled = self._scale_to_range(np.array([risk_score]), min_val=1, max_val=5)[0]

            # Get SHAP values for this department
            dept_shap = shap_values[i]

            # Create mapping of feature names to SHAP values
            feature_shap = {name: abs(value) for name, value in zip(self.feature_names, dept_shap)}

            # Sort features by absolute SHAP value (impact on prediction)
            sorted_features = sorted(feature_shap.items(), key=lambda x: x[1], reverse=True)

            # Get top contributing features
            top_features = sorted_features[:top_n]

            # Calculate total contribution of top features
            total_contribution = sum(abs(value) for _, value in top_features)

            # Format explanation
            feature_contributions = []
            for name, value in top_features:
                # Calculate percentage contribution
                pct = (abs(value) / total_contribution * 100) if total_contribution > 0 else 0
                feature_contributions.append({
                    'feature': name,
                    'contribution_pct': round(pct)
                })

            # Add to explanations list
            explanations.append({
                'department': dept,
                'risk_score': round(risk_score_scaled, 1),
                'top_factors': feature_contributions
            })

        return explanations

    def plot_feature_importance(self):
        """
        Plot feature importance from the trained model

        Returns:
            matplotlib.figure.Figure: The created figure
        """
        if self.model is None:
            print("Model not trained. Please train the model first.")
            return None

        # Get feature importance
        importance = self.model.feature_importances_

        # Create DataFrame for plotting
        importance_df = pd.DataFrame({
            'Feature': self.feature_names,
            'Importance': importance
        }).sort_values('Importance', ascending=False)

        # Create plot
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=importance_df, ax=ax)
        ax.set_title('Feature Importance')
        ax.set_xlabel('Importance')
        ax.set_ylabel('Feature')

        return fig


def run_predictive_modeling(dept_df, label_col=None):
    """
    Run the complete predictive modeling pipeline

    Args:
        dept_df (pd.DataFrame): Department-level aggregated data
        label_col (str, optional): Name of column containing actual risk labels (if available)

    Returns:
        tuple: (risk_scores_df, shap_explanations) - Risk scores and explanations
    """
    print("Starting predictive modeling pipeline...")
    model = PredictiveModel()

    # Prepare features
    X, y, departments = model.prepare_features(dept_df, label_col)

    # Train model if labels are available
    if y is not None:
        model.train_model(X, y)
        model.evaluate_model(X, y)

    # Predict risk scores
    risk_scores = model.predict_risk_scores(X, departments)

    # Generate SHAP explanations
    if model.model is not None:
        explanations = model.get_shap_explanations(X, departments)
    else:
        explanations = None

    return risk_scores, explanations

# Example usage (integrated with previous stages)
if __name__ == "__main__":
    # This would be replaced with actual file processing when running the code
    sample_file_path = "Sample.xlsx"

    # Run preprocessing (from Stage 1)
    processed_data, _ = main(sample_file_path)

    # Run NLP pipeline (from Stage 2)
    _, department_data = process_nlp_pipeline(processed_data)

    # Run predictive modeling
    risk_scores, explanations = run_predictive_modeling(department_data)

    # Display sample results
    print("\nDepartment risk scores:")
    print(risk_scores.head())

    if explanations:
        print("\nSample explanation for", explanations[0]['department'])
        for factor in explanations[0]['top_factors']:
            print(f"- {factor['feature']}: {factor['contribution_pct']}%")